# 201019 딥러닝 스터디 발제

링크: [https://www.notion.so/201019-34a0012fc9654fc6b45c0fb0c0fbb963](https://www.notion.so/201019-34a0012fc9654fc6b45c0fb0c0fbb963)

## 5.4 ELMo(Embeddings from Language Model)

- Allen AI와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법
- 컴퓨터 비전 분야에서 널리 쓰이고 있던 **전이 학습(transfer learning)**을 자연어 처리에 접목
- 전이 학습 : 이미 학습된 모델을 다른 딥러닝 모델의 입력값 또는 부분으로 재사용하는 기법
- ELMo 이후로 사전 학습(Pretraining)된 모델을 각종 다운스트림 태스크(구체적인 문제)에 적용하는 양상이 일반화

    → BERT, GPT 등에 적용

- 사전 학습된 모델을 다운스트림 태스크에 맞게 업데이트하는 과정은 파인 튜닝(fine-tuning)이라고 함
- 단어 시퀀스가 얼마나 자연스러운지 확률값을 학습

    ![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled.png)

    출처: 딥 러닝을 이용한 자연어 처리 입문([https://wikidocs.net/33930](https://wikidocs.net/33930))

- 입력 단어 시퀀스 다음에 어떤 단어가 올지 맞히는 과정에서 학습
- ELMo는 크게 세 가지 요소로 구성

    1) 문자 단위 CNN(Convolutional Neural Network): 각 단어 내 **문자들** 사이의 의미적, 문법적 관계 도출

    2) 양방향 LSTM 레이어: **단어들** 사이의 의미적, 문법적 관계 추출

    3) ELMo 레이어: CNN 출력 벡터, LSTM 레이어 출력 벡터 등을 가중합하는 방식으로 계산

- 문자 단위 CNN과 양방향 LSTM 레이어는 **사전 학습** 과정에서 학습이 됨
- ELMo 레이어는 구체적인 **다운스트림 태스크**를 수행하는 과정에서 학습

### 5.4.1 문자 단위 컨볼루션 레이어

 1) 단어의 임베딩

- ELMo의 입력은 문자, 구체적으로 말하면 문자에 대응하는 유니코드 ID

    ex)         밥          =            ㅂ(초성) , ㅏ, ㅂ(종성)                =             235, 176, 165

- ELMo는 추가적으로 아래와 같은 처리를 함

    ![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%201.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%201.png)

                  참조: <한국어 임베딩> 208p 그림 5-15 'ELMo의 CNN 입력값 만들기'

    - 단어의 시작을 의미하는 스페셜 토큰 <BOW>, 단어의 끝을 알리는 <EOW>에 해당하는 ID를 밥의 유니코드 앞 뒤로 붙임
    - 문자 임베딩 행렬에서 각각의 ID에 해당하는 행 벡터(char dim)를 참조해 붙임
    - 사용자가 정한 max_characters_per_token보다 작을 경우 그 차이만큼을 <PAD>에 해당하는 행 벡터로 채움

2) 컨볼루션 신경망 작업 과정

- 문자 단위 컨볼루션 신경망은 임베딩 행렬을 활용하여 문자 사이의 의미적, 문법적 관계를 추출하며 아래와 같은 작업 과정을 거침

    ![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%202.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%202.png)

         참조: <한국어 임베딩> 209p 그림 5-16 'ELMo character CNN'

    - 위 그림의 컨볼루션 필터 크기는 3 X 문자 임베딩 차원 수(책은 2 X 문자 임베딩 차원 수로 되어 있는데 오기로 보임)
    - 한번 연산할 때마다 세 개의 문자를 보게 됨
    - <BOW>, 밥1, 밥2를 보고 피쳐맵의 **첫 번째 벡터(초록색 값)**를 만들어 냄
    - 밥1, 밥2, 밥3을 보고 피쳐맵의 **두 번째 벡터(빨간색 값)**를 만들어 냄
    - 필터가 슬라이딩하면서 계속해서 피쳐맵의 벡터를 만들어 냄
    - 피쳐맵이 만들어지면 피쳐맵에서 최댓값 하나만 뽑아 **풀링 벡터**의 **첫 번째 값**을 만들어 냄
    - 같은 크기의 컨볼루션 필터를 하나 더 만들고 동일한 과정을 거쳐서 **풀링 벡터**의 두 번째 값을 만들어 냄
    - 사용자가 지정한 횟수만큼 반복하여 **풀링 벡터** 완성
- ELMo의 original 모델에서는 [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]]의 컨볼루션 필터([컨볼루션 필터에서 보는 문자 수, 피쳐 맵 수(=풀링 벡터 차원 수)])를 모두 활용

3) 하이웨이 네트워크와 차원 조정

![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%203.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%203.png)

참조: <한국어 임베딩> 210p 그림 5-17 'ELMo 양방향 LSTM 레이어 입력값 만들기'

- 위 과정을 통해서 만들어진 풀링 벡터들을 모두 이어 붙인 벡터를 만들어 냄
- ELMo의 original 모델의 경우 32+32+64+128+256+512+1024 = 2048의 차원을 가진 벡터가 만들어짐
- 이처럼 문자 단위 컨볼루션 레이어를 통과한 단어 임베딩의 차원 수가 지나치게 크기 때문에 학습에 문제가 생길 수 있음
- 이를 해결하기 위해서 하이웨이 네트워크와 차원 조정을 문자 단위 컨볼루션 레이어의 계산 과정 마지막에 추가한 것으로 보임
    - 하이웨이 네트워크 : 입력 값이 바로 통과 될 수 있는 우회 경로. 깊은 모델에서 그래디언트 전파가 비효율적인 것을 해결해 줌. 입력값을 얼마나 변형할지를 결정하는 **T(변형 게이트)**, 입력값을 얼마나 변형하지 않을지 결정하는 **C(1-T, 캐리 게이트)**를 두어 때에 따라선 입력값이 특정 레이어를 건너 뛰고 다른 레이어로 갈 수 있게 만들어 줌.

**→ 위 과정을 모두 거쳐서 최종적으로 만들어진 임베딩에는 해당 단어 내의 문자들 사이의 의미적, 문법적 관계가 함축되어 있음. 이렇게 만들어진 임베딩들은 양방향 LSTM 레이어의 입력값이 됨.**

### 5.4.2 양방향 LSTM, 스코어 레이어

![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%204.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%204.png)

            참조: <한국어 임베딩> 213p 그림 5-18 'ELMo 양방향 LSTM 및 출력 레이어'

1) LSTM 레이어

- 문자 단위 컨볼루션 신경망을 통과한 단어 벡터들은 두 개의 LSTM 레이어(순방향 LSTM 레이어, 역방향 LSTM 레이어)에 똑같이 입력
- 순방향, 역방향 레이어는 각각 n개의 LSTM 레이어로 구성(기본 모델은 2개로 설정)
- 사용자가 주는 옵션에 따라서 각 LSTM 레이어 사이에 차원 조정 실시할 수도 있음
    - LSTM의 히든 벡터 크기가 1024차원 → 128차원 → 다음 LSTM 히든 벡터 크기 1024차원
- ELMo의 LSTM 레이어에는 레지듀얼 커넥션 기법도 적용하여 일부 계산 노드를 건너뛸 수 있음 → 효율적인 그래디언트 전파

    ![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%205.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%205.png)

    출처: IT찬니 - IT Repository([https://itrepo.tistory.com/36](https://itrepo.tistory.com/36))

- ELMo 모델이 프리트레인할 때는 다음 단어가 무엇인지 맞춰야 함
- 순방향이라면 '꿈'이 주어지면 '보다'를 맞춰야 함
- 단어 하나씩 슬라이딩해 가면서 그 다음 단어가 무엇인지 맞추는 과정을 반복하다 보면 문장 내에 속한 단어들 사이의 의미적, 문법적 관계들을 ELMo 모델이 이해할 수 있게 됨

2) 손실 레이어

- 단어를 맞추는 역할을 하는 레이어
- 출력 히든 벡터를 선형변환한 뒤 소프트맥스를 취함 ex) [0.2, 0.3, 0.5]
- 확률 벡터와 정답 단어에 해당하는 인덱스만 1인 원핫벡터로 크로스 엔트로피 계산 ex) [0, 0, 1]
- 크로스 엔트로피를 최소화하는 방향으로 모델 전체를 조금씩 업데이트

    ![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%206.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%206.png)

    $y_j$ : 실제값,  $p_j$ : 예측값

    ```python
    import numpy as np
    p_1 = [0.1, 0.8, 0.1]
    p_2 = [0.1, 0.1, 0.8]
    y = [0, 0, 1]
    cross_entropy_1 = -np.sum(y*np.log(p_1))
    cross_entropy_2 = -np.sum(y*np.log(p_2))
    print('cross_entorpy_1 : ', cross_entropy_1)
    print('cross_entorpy_2 : ', cross_entropy_2)
    # cross_entropy_1 : 2.3025850929940455
    # cross_entropy_2 : 0.2231435513142097
    ```

- 소프트맥스 확률을 구할 때 일부 단어들만 샘플링해서 구함
    - 10만 개 이상의 어휘 집합을 가짐. 한 번 업데이트할 때마다 10만 개 단어 전체를 대상으로 소프트맥스 확률을 계산하면 계산량이 매우 많아지고 메모리가 부족할 수 있기 때문에 샘플링 방식을 취함
    - 오답 단어(네거티브 샘플)를 전체 단어에서 일부 샘플하고 이를 정답 단어(포지티브 샘플)와 함께 소프트맥스 확률을 계산

        ex) 꿈, 보다, 해몽(포지티브 샘플) + 집, 고양이, 강아지(네거티브 샘플)

- 프리트레인 단계에서는 순방향, 역방향 네트워크를 별개의 모델로 보고 서로 다른 학습 데이터 입력
    - 손실 레이어에서는 순방향, 역방향 LSTM 출력 히든 벡터를 더하거나 합치지 않고 각각의 레이블을 맞추는 것을 독립적으로 학습
    - 순방향(역방향)일 때는 뒤(앞)의 정답을 모델에 알려주면 안되기 때문에 독립이 필요함
- 프리트레인이 끝나면 손실 레이어는 더 이상 사용하지 않음

### 5.4.3 ELMo 레이어

- ELMo 레이어의 임베딩은 프리트레인이 끝나고 구체적인 다운스트림 태스크를 학습하는 과정에서 도출
- 임의의 태스크를 수행하기 위한 문장 k번째 토큰의 ELMo 임베딩은 다음 수식과 같음

                                                 $ELMo _{k}^{task} = \gamma^{task} \displaystyle\sum_{j=0}^L s _{j}^{task} \bold h _{k, j}^{LM}$

    - $\bold h _{k, j}^{LM}$는 k번째 토큰의 j번째 레이어의 순방향, 역방향 LSTM 히든 벡터를 이어 붙인 벡터
    - $s _{j}^{task}$는 j번째 레이어가 해당 태스크 수행에 얼마나 중요한지를 가리키는 스칼라 값
    - $\gamma^{task}$는 ELMo 벡터의 크기를 스케일해 해당 태스크 수행을 돕는 역할
    - ELMo 임베딩은 각 레이어별 히든 벡터들에 $s _{j}^{task}$로 가중합을 한 결과
    - ELMo 모델은 입력 문장의 토큰 수만큼의 토큰 임베딩을 반환
    - L은 양방향 LSTM 레이어 수를 가리킴(보통 2)
    - j=0일 때는 문자 단위 컨볼루션 레이어 출력, j=1일 때는 양방향  LSTM 레이어의 첫 번째 레이어 출력, j=2일 때는 두 번째 레이어 출력을 의미
- 아래 그림은 위의 수식을 시각화한 그림

![201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%207.png](201019%20%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%A5%E1%84%83%E1%85%B5%20%E1%84%87%E1%85%A1%E1%86%AF%E1%84%8C%E1%85%A6%205cc32173c9d049ad98e3284bf68e0719/Untitled%207.png)

                        참조: <한국어 임베딩> 215p 그림 5-19 'ELMo 임베딩'

- ELMo 임베딩으로 문서 분류라는 태스크를 수행한다고 하면  $s _{j}^{task}$는 아래 그림에서 동그라미들의 색상을 가리킴
- 색상이 진할수록 가중치가 높음. 위의 그림에서는 문자 단위 컨볼루션 레이어의 출력 벡터들이 가장 중요
- 학습 손실을 최소화하는 방향으로  $s _{j}^{task}$ 파라미터가 업데이트

### 5.4.4 프리트레인 튜토리얼

- 프리트레인에서는 문자 단위 컨볼루션 레이어와 양방향 LSTM, 레이어만을 학습

1) 데이터 준비

- ELMo 모델을 프리트레이닝하기 위해서는 형태소 분석이 완료된 데이터가 필요
- 튜토리얼에서는 Mecab으로 토크나이즈한 한국어 위키백과, 네이버 영화 리뷰 말뭉치, KorQuAD 데이터 세개를 합쳐 사용
- 효율적 학습을 위해 말뭉치를 10만 개 라인으로 나누어줌

```bash
#코드 5-28 학습 데이터 준비
git pull origin master
bash preprocess.sh dump-tokenized
cd /notebooks/embedding
mkdir -p data/sentence-embeddings/elmo/pretrain-ckpt/traindata
# 통합
cat data/tokenized/wiki_ko_mecab.txt data/tokenized/ratings-mecab.txt data/tokenized/korquad_mecab.txt 
> data/tokenized/corpus_mecab.txt
# 10만개씩 라인 나눠서 저장
split -l 100000 data/tokenized/corpus_mecab.txt data/sentence-embeddings/elmo/pretrain-ckpt/traindata/data_
```

- 프리트레인 위해서는 어휘 집합 필요
    - 어휘 집합이 쓰이는 경우는 프리트레인 입력 단계와 프리트레인 예측 단계
    - 프리트레인 입력 단계에서의 어휘 집합 활용
        - 프리트레이닝 과정에서 매 스텝마다 다량의 문서를 문자 단위 정수 ID 시퀀스 형태로 변환하는 것은 비효율적
        - ELMo 모델은 프리트레이닝을 시작하기 전에 어휘 집합에 포함된 각 단어를 모두 문자 단위 ID 시퀀스로 미리 변환
        - 문장 내 단어들이 미리 만들어 놓은 리스트에 포함되어 있으면 꺼내어 쓰고, 없으면 새롭게 ID 시퀀스로 변환하여 활용
    - 프리트레인 예측 단계에서의 어휘 집합 활용
        - ELMo 모델은 주어진 단어 시퀀스 다음의 단어를 맞추는 과정에서 학습
        - 즉, 예측 결과(출력)가 단어
        - 문자 시퀀스를 입력으로 받은 뒤 단어를 출력해야 함으로 어휘 집합이 반드시 필요

```bash
#코드 5-29 ELMo 어휘 집합 구축
cd /notebooks/embedding
python models/sent_utils.py --method construct_elmo_vocab \
--input_path data/tokenized/corpus_mecab.txt \
--output_path data/sentence-embeddings/elmo/pretrain-ckpt/elmo-vocab.txt
```

2) 하이퍼파라미터 설정

- ELMo 모델의 하이퍼파라미터는 크게 문자 단위 컨볼루션 레이어, 양방향 LSTM 레이어 두개로 나누어 생각할 수 있음
- 문자 단위 컨볼루션 레이어는 char_cnn, 양뱡향 LSTM 레이어는 lstm 부분에서 하이퍼파라미터 설정할 수 있음
- 아래 하이퍼파라미터들은 ELMo 기본 모델 대비 차원 수나 레이어 수를 줄인 커스텀 모델

```python
#코드 5-30 ELMo 하이퍼파라미터 설정(/notebooks/embedding/models/train_elmo.py 일부)
options = {
	'bidirectional' : True, # 양방향 LSTM 레이어 적용 여부
	'char_cnn' : {'activation' : 'relu', # 컨볼루션 필터 가중치 초기화, 맥스 풀링 이후 활성화 함수(relu or tanh)
								'embedding': {'dim' : 16}, # 문자별 임베딩 행렬의 차원 수
								'filters': [[1, 32], # [문자 몇 개씩 볼지, 필터 수]
														[2, 32],
														[3, 64],
														[4, 128],
														[5, 256],
														[6, 512],
														[7, 1024]],
								'max_characters_per_token' : 30, # 한 토큰에 최대 몇 개의 문자 입력할지
								'n_characters' : 261, # 사용하는 문자 종류 개수(유니코드 ID 수, 261개 고정)
								'n_highway' : 2}, # 하이웨이 네트워크 적용 횟수
	'dropout' : 0.1, # 드롭아웃 비율
	
	'lstm' : {
		'cell_clip' : 3, # LSTM 셀의 값의 제한 크기, 그래디언트 문제 막기 위함
		'dim' : 1024, # 셀의 입출력 벡터 차원 수
		'n_layers' : 2, # 양방향 LSTM 레이어의 개수
		'proj_clip' : 3, # LSTM 셀의 값의 제한 크기, 그래디언트 문제 막기 위함
		'projection_dim' : 128, # 셀의 입출력 벡터 차원 수
		'use_skip_connections' : True}, # 레이어와 레이어 사이를 건너뛰는 커넥션 유무(residual connection)

	'all_clip_norm_val' : 10.0, # 역전파 그래디언트 크기

	'n_epochs' : 10, # 에폭 수
	'n_train_tokens' : n_train_tokens, # 에폭 수를 정할 때 참고하는 데이터 크기(전체 토큰 수)
	'batch_size' : 128, # 배치 데이터 크기
	'n_tokens_vocab': 100000, # 어휘 집합 크기
	'unroll_steps' : 20, # 몇 개의 단어 시퀀스 예측할지
	'n_negative_samples_batch' : 8192, # 한 스텝에서 계산하는 네거티브 샘플 개수
}
```

3) ELMo 프리트레이닝

- 아래 코드를 통해서 ELMo 프리트레이닝 시작할 수 있음
- 정해진 에폭이 다 돌 때까지 백그라운드에서 학습하며 log 파일에 학습 과정이 기록됨
- ELMo 모델은 GPU 환경에서만 프리트레인할 수 있음

```bash
#코드 5-31 ELMo 프리트레이닝
cd /notebooks/embedding
nohup sh -c "python models/train_elmo.py --train_prefix 'data/sentence-embeddings/elmo/pretrain-ckpt/traindata/* 
--vocab_file data/sentence-embeddings/elmo/pretrain-ckpt/elmo-vocab.txt --save_dir data/sentence-embeddings/elmo/pretrain-ckpt 
--n_gpus 1" > elmo-pretrain.log &
```

- 학습이 완료되면 모델의 파라미터 등이 기록된 체크포인트 파일이 save_dir에 저장
- 체크포인트 파일이 있는 상태에서 코드 5-32를 실행하면 파인 튜닝을 할 수 있는 파일 형태(h5)로 저장할 수 있음
- ELMo 레이어에 대한 파인 튜닝은 6장을 참고

```bash
#코드 5-32 ELMo 모델 저장
cd /notebooks/embedding
python models/sent_utils.py --metohod dump_elmo_weights \
--input_path data/sentence-embeddings/elmo/pretrain-ckpt \
--output_path data/sentence-embeddings/elmo/pretrain-ckpt/elmo.model
```